# Laundering Money Prediction
This model tries to automate the manual decision process of a risk analyst to identify a possible case of money laundering from the events automatically raised by a software based on the behavior of the users of a platform. The model takes into account features such as the amount of dollars the user receives (*total_income_dollar_amount*), the amount of dollars the user sends (*total_outcome_dollar_amount*), and a risk scale automatically assigned to the user (risk_pld). From these numerical and categorical data, the current model must assign a probability (0-1) that the evaluated user is involved in money laundering.

Below I present a demo explaining the architecture of the solution and showing how it works.

https://www.loom.com/share/a99075eb512b45258db3181dec24b55d

## Architecture
![](https://i.ibb.co/m53pk9T/Model-store-and-model-registry.png)

## Features
- [x] All the project dependencies and its versions can be found on the *requirements.txt* file.
- [x] The model can be used as a service or a ***Live scoring Model*** which lives in an EC2 instance on AWS
- [x] The model is served through a *Flask* Web Application using *Nginx* as a Web Server and *Guinicorn* as a WSGI HTTTP Server
- [x] Every training of the model generates a new experiment and registers a new version of the model in the model Registry at *MLFlow*.

[![MLFlow as model store and model registry](https://i.ibb.co/GxZk7dc/Captura-de-pantalla-2023-08-06-a-la-s-6-49-19-a-m.png "MLFlow as model store and model registry")](https://i.ibb.co/GxZk7dc/Captura-de-pantalla-2023-08-06-a-la-s-6-49-19-a-m.png "MLFlow as model store and model registry")

- [x] MLFlow is also running on a separate *EC2* instance, so you can see the list of experiments of the model training [here](http://ec2-18-206-253-206.compute-1.amazonaws.com:5000/#/experiments/3?searchFilter=&orderByKey=attributes.start_time&orderByAsc=false&startTime=ALL&lifecycleFilter=Active&modelVersionFilter=All%20Runs&selectedColumns=attributes.%60Source%60,attributes.%60Models%60,attributes.%60Dataset%60&compareRunCharts= "here") and the registered models [here](http://ec2-18-206-253-206.compute-1.amazonaws.com:5000/#/models/consumers-laundering-model "here"). By default, the last trained model will be promoted as the "Productive model", it means that the last trained model will be used by the EC2 instance with the web application to make predictions with the incoming requests data. Previous productive models will be assigned a stage None.
- [x] MLFlow is storing its data on a *RDS instance* and all the generated artifcats during training and experimentation are saved on a *S3 Bucket* called "mlflow-artifact-remote-isa".

[![MLFlow RDS ](https://i.ibb.co/qDX3cYC/Captura-de-pantalla-2023-08-06-a-la-s-6-52-12-a-m.png "MLFlow RDS ")](https://i.ibb.co/qDX3cYC/Captura-de-pantalla-2023-08-06-a-la-s-6-52-12-a-m.png "MLFlow RDS ")

[![MLFlow artifacts storage on S3](https://i.ibb.co/tztybr0/Captura-de-pantalla-2023-08-06-a-la-s-6-51-28-a-m.png "MLFlow artifacts storage on S3")](https://i.ibb.co/tztybr0/Captura-de-pantalla-2023-08-06-a-la-s-6-51-28-a-m.png "MLFlow artifacts storage on S3")

- [x] The model is a *XGBClassifier* which uses preprocessed data in a pipeline union (numeric and categorical data). This preprocessing has been customed through the use of *Sklearn* custom transformers. You can find all the custom transformers on the transformers.py file.
- [x] Hyperparamers, model settings, target column, numerical and categorical feature column names and other constant values can be found on the settings.py file.
- [x] I have set values for secrets as AWS Credentials, Bucket Names, Hosts, Paths and Prefect Keys on an .env file that is not available in the repo for security reasons.
- [x] **Pre-commit checks include: ** Detect private keys and aws credentials and styling rules as trailing whitespace, black, isort and pylint.
- [x] Everytime you send a POST request to the *Live Scoring Model* or *Online Model* to get a prediction you will get the result or results of the probability of money laundering for each case and the body response will also include the model metadata so can be sure about the model that was used to perform the prediction, this values can be compared to the ones saved on MLFlow.
- [x] *Evidently reports* were used to check Data Quality during each training. The *DataQualityPreset* checks the amount of columns, rows, nan values, unique values among other metrics that are stored on a *PostgreSQL* DB living at an EC2 instance.
- [x] Docker compose file was used to create two containers inside the EC2 instance with the model to save and serve the data generated by the *Evidently Reports*. Each training sends this data to the EC2 instance to save it on the DB and serve it through [*Adminer*](http://54.173.97.152:8080/?pgsql=db&username=postgres&db=test&ns=public&select=metrics_summary "adminer"). You can also visualize the data on a [*Grafana Dashboard*](http://54.173.97.152:3000/d/f44bffaa-2c4d-4eec-b994-cb4fcf670935/mlops-final-project?orgId=1 "Grafana Dashboard") to get more relevant data for each version of the registered model.

[![Adminer](https://i.ibb.co/D1SD9xC/Captura-de-pantalla-2023-08-06-a-la-s-6-49-39-a-m.png "Adminer")](https://i.ibb.co/D1SD9xC/Captura-de-pantalla-2023-08-06-a-la-s-6-49-39-a-m.png "Adminer")

[![Grafana Dashboard](https://i.ibb.co/5GxCv18/Captura-de-pantalla-2023-08-06-a-la-s-6-49-08-a-m.png "Grafana Dashboard")](https://i.ibb.co/5GxCv18/Captura-de-pantalla-2023-08-06-a-la-s-6-49-08-a-m.png "Grafana Dashboard")

- [x] A *Custom Alert* was set on *Grafana* to notify about ROC AUC values on test dataset below 0.89.
- [x] *Makefile* runs black and isort checks before starting a new training of the model through the run.sh file.
- [x] run.sh file sets the TRACKING_SERVER_HOST environment variable and runs the train.py file to get a new trained version of the model.
- [x] Deployments and training pipeline are handled using *Prefect Cloud*. It means you can follow all the steps of the training on the flow runs screen and create a new deployment from the front of the Prefect web app. Prefect cloud was configured on the EC2 instance, however, due to timeout reasons I will not use it on the demo to trigger deployments, but locally.
- [x] Data used for the model training is being downloaded from S3 using the *boto3* module. This data cannot be published for reasons of privacy of the source.
- [x] The Live scoring model with Gunicorn and the docker containers running Grafana and Adminer are being executed as services inside the EC2 instance that can be started, restarted or stopped through the *systemctl* command.

## Tools used

Tool  | Use Case
------------- | -------------
Flask | Python web service to process the POST requests, extract the body data, call the current productive model on MLFlow, perform the predictions using it and return the response to the user.
NGINX | Web Server Container to process incoming connections on port 80.
Gunicorn | WSGI HTTP Server.
Docker | To containerize the PostgreSQL database service and grafana dashboards.
Adminer | To access PostgreSQL data saved from Evidently Reports and AUC ROC.
Evidently | To check DataQuality through the DataQualityPreset.
Grafana | Dashboards to show DataQuality reports data by model version and to alert about low ROC AUC values on test dataset.
PostgreSQL | To store Evidently report results.
RDS | To store MLFlow data.
S3 Buckets | To store original data for training and resulting artifacts generated by MLFlow on each experiment.
EC2  | One to serve MLFlow and other one to serve the Live Scoring Model and the Docker containers with Adminer and Grafana.
MLFlow | Model store and Model registry.
Prefect and Prefect Cloud | To handle training runs and deployments.

## Training Flow pipeline
[![Main Training Flow](https://i.ibb.co/q7WSvtk/Captura-de-pantalla-2023-08-06-a-la-s-6-50-47-a-m.png "Main Training Flow")](https://i.ibb.co/q7WSvtk/Captura-de-pantalla-2023-08-06-a-la-s-6-50-47-a-m.png "Main Training Flow")

**1. Data Generation**:
- Download data from S3 Bucket.
- Split data into train and test datasets.
- Validate that resulting dataframes are not empty.

**2. Train Laundering Money Model**:
- Creation of Pipeline with custom transformers, feature union and GridSearch.
- Fit the model Pipeline using the data generated previously.
- Calculate ROC AUC metrics for both datasets (train and test).

**3. Generate Evidently Reports**:
- Prepare DB to save the data. It creates a connection with the EC2 instance to verify the existence of the table that will store the data.
- **3.1 Generate Report:**
 - Run the DataQualityPreset report on the train (reference values) and test (current values) data using a custom column mapping.
 - Save results of the report as well as the ROC AUC calculated previously, on the PostgreSQL DB living in the Docker container at the EC2 instance.

**4. Save last trained model**: Creates a new version of the model in the model registry at MLFlow and set its stage as **Production** to be used by the Flask Application.


 ## Links
 Credentials to access Adminer:

 DB Type  |  User  | Password | DB Name
------------- | -------------  | ------------- | -------------
PostgreSQL | postgres | example | test

 Credentials to access Grafana:

User  | Password
------------- | -------------
admin | 123456


- MLFlow on EC2 instance - [List of experiments](http://ec2-18-206-253-206.compute-1.amazonaws.com:5000/#/experiments/3?searchFilter=&orderByKey=attributes.start_time&orderByAsc=false&startTime=ALL&lifecycleFilter=Active&modelVersionFilter=All%20Runs&selectedColumns=attributes.%60Source%60,attributes.%60Models%60,attributes.%60Dataset%60&compareRunCharts= "List of experiments").
- MLFlow on EC2 instance - [List of Model versions ](http://ec2-18-206-253-206.compute-1.amazonaws.com:5000/#/models/consumers-laundering-model "List of Model versions ")(Model Registry).
- Online Model or Live Scoring Model endpoint to perform predictions: 54.173.97.152/predict.
- [Adminer](http://54.173.97.152:8080/?pgsql=db&username=postgres&db=test&ns=public&select=metrics_summary "Adminer") - Results of ROC AUC and Data Quality for each registered version of the model.
- [Grafana Dashboard](http://54.173.97.152:3000/d/f44bffaa-2c4d-4eec-b994-cb4fcf670935/mlops-final-project?orgId=1 "Grafana Dashboard").
- [Grafana Alert](http://54.173.97.152:3000/alerting/grafana/b26e392b-6cb2-4c5b-b933-74475f715e92/view "Grafana Alert") to notify low AUC ROC on test dataset.


 ## Environment Variables


| Key  | Value  |
| :------------ | :------------ |
| TRACKING_SERVER_HOST  | ec2-18-206-253-206.compute-1.amazonaws.com  |
| DB_HOST  | 54.173.97.152  |

## Instructions

To install all the required dependencies you should have a Python environment already created (3.11 version). Then run from the root folder of the project:

`$ pip install -r requirements.txt`

To run one experiment and create a new version of the model on MLFlow and automatically serve it on the EC2 instance run:

`$ make run_train` or just `$ python train.py`

**Note: **The demo video illustrates how it works, however, since the data used to train the model has privacy policies you will not be available to use it locally.

To run a worker to receive deployments from Prefect:

`$ prefect worker start --pool 'mlops-project-pool'`


## Try it on Postman!
You can test the online model on the EC2 instance by sending a POST request using Postman with a body that follows the structure shown below:

```json
{
    "total_outcome_dollar_amount": [2109.19],
    "total_income_dollar_amount": [1305.06],
    "risk_pld": ["HIGH"]
}
```

[![Run in Postman](https://run.pstmn.io/button.svg)](https://app.getpostman.com/run-collection/27355088-b1ac577c-ab4e-4ba8-bacf-6c5977ec7587?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D27355088-b1ac577c-ab4e-4ba8-bacf-6c5977ec7587%26entityType%3Dcollection%26workspaceId%3D440f6040-36f5-4f4a-bc6d-5f82b4f8c959)
